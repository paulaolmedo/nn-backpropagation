{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP IAR 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from random import seed\n",
    "from random import random\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciones de activación globales, para ser utilizadas más adelante\n",
    "def tanh(activation):\n",
    "    return np.tanh(activation)\n",
    "\n",
    "def tanh_derivative(output):\n",
    "    return (1.0 - (output*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Define la estructura básica de neurona, con su respectivo peso\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None):\n",
    "        if not weight:\n",
    "            self.weight = random()\n",
    "        else:\n",
    "            self.weight = weight\n",
    "            \n",
    "    def __str__(self):\n",
    "        return str(self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Define la unión de las neuronas\n",
    "    \"\"\"\n",
    "    def __init__(self, n, neurons=None):\n",
    "        if not neurons:\n",
    "            self.neurons = []\n",
    "            for i in range(n):\n",
    "                self.neurons.append(Neuron())\n",
    "        else:\n",
    "            self.neurons = neurons\n",
    "\n",
    "        self.output = None \n",
    "        self.delta = None \n",
    "        \n",
    "    def __str__(self):\n",
    "        neurons_print = \"neurons = [\"\n",
    "        for neuron in self.neurons:\n",
    "            neurons_print = neurons_print + str(neuron) + \", \"\n",
    "        neurons_print = neurons_print[:-2] + \"]\\n\"\n",
    "        output_print = \"output = {}\\n\".format(self.output)\n",
    "        delta_print = \"delta = {}\\n\".format(self.delta)\n",
    "\n",
    "        return neurons_print + output_print + delta_print\n",
    "                       \n",
    "    def activate(self, layer, inputs):\n",
    "        activation = layer[-1].weight\n",
    "        for i in range(len(layer) -1):\n",
    "            activation += layer[i].weight * inputs[i]\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        \"\"\"\n",
    "        arguments: shape se definirá como un diccionario,\n",
    "               que tiene como elementos el tamaño de la red\n",
    "        \"\"\"\n",
    "        self.layers = list()\n",
    "        self._init_layers(shape)\n",
    "        \n",
    "    def _init_layers(self, shape):\n",
    "        \"\"\"\n",
    "        inicializa las capas según los valores definidos por el usuario\n",
    "        \"\"\"\n",
    "        self.hidden_layer = []\n",
    "        for i in range(shape[\"hidden\"]):\n",
    "            self.hidden_layer.append(Layer(shape[\"inputs\"] + 1))\n",
    "        \n",
    "        self.output_layer = []\n",
    "        for i in range(shape[\"outputs\"]):\n",
    "            self.output_layer.append(Layer(shape[\"hidden\"] + 1))\n",
    "    \n",
    "        self.layers.append(self.hidden_layer)\n",
    "        self.layers.append(self.output_layer)\n",
    "            \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        muestra estructura red\n",
    "        \"\"\"\n",
    "        out = \"\"\n",
    "        for super_layer in self.layers:\n",
    "            out = out + \"---- complete layer ----\\n\"\n",
    "            for sub_layer in super_layer:\n",
    "                out = out + \"-- sub layer --\\n\"\n",
    "                out = out + str(sub_layer)\n",
    "        return out    \n",
    "        \n",
    "    def forward_propagate(self, net_inputs):\n",
    "        \"\"\"\n",
    "        cálculo de la salida (hacia adelante) de las capas hasta el final\n",
    "        \"\"\"\n",
    "        inputs = net_inputs\n",
    "        for super_layer in self.layers:\n",
    "            temporal_output = []\n",
    "            for sub_layer in super_layer:\n",
    "                activation = sub_layer.activate(sub_layer.neurons, inputs)\n",
    "                output = tanh(activation) #sigmoid(activation) #tanh(activation)\n",
    "                sub_layer.output = output\n",
    "                temporal_output.append(output)\n",
    "\n",
    "            inputs = temporal_output\n",
    "        return inputs\n",
    "        \n",
    "    def backward_propagate_error(self, expected):\n",
    "        \"\"\"\n",
    "        cálculo de los errores hacia atrás\n",
    "        \"\"\"\n",
    "        net_range = len(self.layers)\n",
    "        \n",
    "        for i in reversed(range(net_range)):\n",
    "            layer = self.layers[i]\n",
    "            errors = list()\n",
    "            \n",
    "            layer_range = len(layer)\n",
    "            \n",
    "            if i != (net_range - 1):         \n",
    "                for j in range(layer_range):\n",
    "                    error = 0.0\n",
    "                    for element in self.layers[i+1]:\n",
    "                        error += element.neurons[j].weight * element.delta\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(layer_range):\n",
    "                    element = layer[j]\n",
    "                    output = element.output\n",
    "                    errors.append(expected[j] - output)\n",
    "                \n",
    "            for j in range(layer_range):\n",
    "                element = layer[j]\n",
    "                element.delta = errors[j] * tanh_derivative(element.output)\n",
    "        \n",
    "    def update_weights(self, net_inputs, learning_rate):\n",
    "        \"\"\"\n",
    "        actualización de los pesos de cada capa\n",
    "        \"\"\"\n",
    "        net_range = len(self.layers)\n",
    "        \n",
    "        for i in range(net_range):            \n",
    "            inputs = net_inputs[:-1]\n",
    "\n",
    "            if i != 0:\n",
    "                inputs = [element.output for element in self.layers[i-1]]\n",
    "                \n",
    "            for element in self.layers[i]:\n",
    "                for j in range(len(inputs)):\n",
    "                    element.neurons[j].weight += learning_rate * element.delta * inputs[j]\n",
    "                element.neurons[-1].weight += learning_rate * element.delta\n",
    "    \n",
    "        \n",
    "    def fit(self, dataset, learning_rate, epochs, n_outputs):\n",
    "        \"\"\"\n",
    "        entrenamiento de la red\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            sum_error = 0\n",
    "            for row in dataset:\n",
    "                outputs = self.forward_propagate(row)\n",
    "                expected = [0 for i in range(n_outputs)]\n",
    "                expected[int(row[-1])] = 1\n",
    "                sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "                self.backward_propagate_error(expected)\n",
    "                self.update_weights(row, learning_rate)\n",
    "    \n",
    "    def predict(self, row):\n",
    "        \"\"\"\n",
    "        predicciones\n",
    "        \"\"\"\n",
    "        outputs = self.forward_propagate(row)\n",
    "        return outputs.index(max(outputs))\n",
    "    \n",
    "    def compute_confusion_matrix(self, y_expected, y_predicted):\n",
    "        \"\"\"\n",
    "        cálculo de la matriz de confusión\n",
    "        \"\"\"\n",
    "        classes = len(np.unique(y_expected))\n",
    "        result = np.zeros((classes, classes))\n",
    "    \n",
    "        for i in range(len(y_expected)):\n",
    "            result[y_expected[i]][y_predicted[i]] += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def save_params(self, nombre=\"params\"):\n",
    "        pickle.dump(self.w, open(nombre + \".pickle\", \"wb\"))\n",
    "        \n",
    "    def load_params(self, nombre=\"params\"):\n",
    "        self.w = pickle.load(open(nombre + \".pickle\"), \"rb\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.21192195 1.52152313 3.32068152 6.7957075  1.        ]\n",
      " [4.64300389 4.11081718 1.07105344 6.3184066  0.        ]\n",
      " [9.53565588 4.66845313 4.64456258 6.89257513 1.        ]\n",
      " ...\n",
      " [3.22427171 4.73382836 3.02922467 5.55866423 0.        ]\n",
      " [6.16262185 2.14275167 8.43693035 3.91897483 0.        ]\n",
      " [4.2134662  7.25175006 8.64138786 4.84727099 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "data_path = './'\n",
    "\n",
    "X = np.genfromtxt(data_path + 'X.csv', delimiter=',')\n",
    "Y_plain = Y = np.genfromtxt(data_path + 'Y.csv', delimiter=',')\n",
    "Y = np.vstack(Y_plain)\n",
    "\n",
    "dataset = np.concatenate([X, Y], axis = 1)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = len(dataset[0]) - 1\n",
    "#print(\"INPUT DIM \")\n",
    "#print(n_inputs)\n",
    "\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "#print(\"OUTPUT DIM \")\n",
    "#print(n_outputs)\n",
    "\n",
    "seed(1)\n",
    "\n",
    "shape = {\n",
    "    \"inputs\": n_inputs,\n",
    "    \"hidden\": 6,\n",
    "    \"outputs\": n_outputs\n",
    "}\n",
    "net = Clasificador(shape) #inicializa la red\n",
    "\n",
    "net.fit(dataset, 0.040, 500, n_outputs) #con learning rate 0.040 y 100 epochs, clasifica 'correctamente' 1821 de 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abro el set de datos inicial nuevamente para convertir los valores a int (eran float)\n",
    "\n",
    "y_data = open('./Y.csv')\n",
    "y_coordinates = csv.reader(y_data, delimiter=',')\n",
    "\n",
    "y_list = []\n",
    "\n",
    "for row in y_coordinates:\n",
    "    y_list.append(row)\n",
    "\n",
    "new_y_list = []\n",
    "for sub in y_list:\n",
    "    new_sub = []\n",
    "    for item in sub:\n",
    "        new_item = float(item)\n",
    "        new_sub.append(int(new_item))\n",
    "    new_y_list.append(new_sub)\n",
    "\n",
    "y_array = np.array(new_y_list)\n",
    "Y = np.hstack(y_array) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizo las predicciones\n",
    "temporal_list = []\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = net.predict(row)\n",
    "    temporal_list.append(prediction)\n",
    "    \n",
    "y_predictions = np.array(temporal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9306.  694.]\n",
      " [ 945. 9055.]]\n"
     ]
    }
   ],
   "source": [
    "matrix  = net.compute_confusion_matrix(Y, y_predictions) #calculo la matriz de confusión\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_negative = matrix[0][0]\n",
    "false_positive = matrix[0][1]\n",
    "false_negative = matrix[1][0]\n",
    "true_positive = matrix[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9288132116114474\n",
      "Accuracy:  0.91805\n",
      "Recall:  0.9055\n",
      "True negative rate:  0.0694\n"
     ]
    }
   ],
   "source": [
    "#precision True Positive/(True Positive + False Positive)\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "accuracy = (true_negative + true_positive)/(true_negative + true_positive + false_positive + false_negative)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "t_negative_rate = false_positive / (false_positive + true_negative) \n",
    "print(\"Precision: \", precision)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"True negative rate: \", t_negative_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación de la cátedra y exportación de resultados y parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizo las predicciones para X_test.csv\n",
    "\n",
    "X = np.genfromtxt(data_path + 'X_test.csv', delimiter=',')\n",
    "\n",
    "temporal_list = []\n",
    "\n",
    "for row in X:\n",
    "    prediction = net.predict(row)\n",
    "    temporal_list.append(prediction)\n",
    "    \n",
    "y_predictions = np.array(temporal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Y_test_Grupo_04.csv', y_predictions, fmt='%.18e', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
